{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52b0657f",
   "metadata": {},
   "source": [
    "## Using Computational Methods to Simulate Spotify's Algorithm to find music characterics such as tempo, danceability, key, loudness,instramentalness and energy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750a266",
   "metadata": {},
   "source": [
    "In this code we are using the librosa python library which is used for audio and music analysis. It provides a wide range of tools for working with audio data, including functions for loading audio files, extracting features, performing transformations, and visualizing audio data. Librosa is widely used in the field of music information retrieval (MIR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bc0ba8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Audio Features:\n",
      "{\n",
      "    \"Tempo\": [\n",
      "        114.84375\n",
      "    ],\n",
      "    \"Loudness\": -22.791458468304516,\n",
      "    \"Key\": \"G0\",\n",
      "    \"Danceability\": [\n",
      "        0.6703728687516529\n",
      "    ],\n",
      "    \"Instrumentalness\": 0.8519929647445679,\n",
      "    \"Energy\": 0.08817879110574722\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Function to calculate danceability\n",
    "def calculate_danceability(y, sr):\n",
    "    # Calculate tempo\n",
    "    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "    \n",
    "    # Calculate beat strength\n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    beat_strength = onset_env.mean()\n",
    "    \n",
    "    # Calculate spectral features\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr).mean()\n",
    "    spectral_flatness = librosa.feature.spectral_flatness(y=y).mean()\n",
    "    \n",
    "    # Normalize features to a scale of 0.0 to 1.0\n",
    "    tempo_norm = min(tempo / 200, 1.0)  # Assuming 200 BPM as a high tempo\n",
    "    beat_strength_norm = min(beat_strength / 1.0, 1.0)  # Assuming beat strength ranges from 0 to 1\n",
    "    spectral_contrast_norm = min(spectral_contrast / 50, 1.0)  # Assuming 50 as a high spectral contrast\n",
    "    spectral_flatness_norm = min(spectral_flatness / 1.0, 1.0)  # Assuming spectral flatness ranges from 0 to 1\n",
    "    \n",
    "    # Combine features to estimate danceability\n",
    "    danceability = (tempo_norm * 0.3) + (beat_strength_norm * 0.4) + (spectral_contrast_norm * 0.2) - (spectral_flatness_norm * 0.1)\n",
    "    danceability = max(0.0, min(danceability, 1.0))  # Ensure the value is between 0.0 and 1.0\n",
    "    return danceability\n",
    "\n",
    "# Function to calculate instrumentalness\n",
    "def calculate_instrumentalness(y, sr):\n",
    "    # Harmonic-Percussive Source Separation (HPSS)\n",
    "    y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "    \n",
    "    # Calculate the ratio of harmonic to percussive energy\n",
    "    harmonic_energy = np.sum(y_harmonic**2)\n",
    "    percussive_energy = np.sum(y_percussive**2)\n",
    "    instrumentalness = harmonic_energy / (harmonic_energy + percussive_energy)\n",
    "    \n",
    "    # Normalize to a scale of 0.0 to 1.0\n",
    "    instrumentalness = max(0.0, min(instrumentalness, 1.0))\n",
    "    return instrumentalness\n",
    "\n",
    "# Function to calculate energy\n",
    "def calculate_energy(y):\n",
    "    # Calculate RMS energy\n",
    "    rms = librosa.feature.rms(y=y).mean()\n",
    "    \n",
    "    # Normalize to a scale of 0.0 to 1.0\n",
    "    energy = min(rms / 1.0, 1.0)  # Assuming RMS energy ranges from 0 to 1\n",
    "    return energy\n",
    "\n",
    "# Function to extract audio features using librosa\n",
    "def extract_audio_features(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "    # Extract features\n",
    "    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "    chroma_stft = float(librosa.feature.chroma_stft(y=y, sr=sr).mean())\n",
    "    rmse = float(librosa.feature.rms(y=y).mean())\n",
    "    spectral_contrast = float(librosa.feature.spectral_contrast(y=y, sr=sr).mean())\n",
    "    spectral_bandwidth = float(librosa.feature.spectral_bandwidth(y=y, sr=sr).mean())\n",
    "    spectral_flatness = float(librosa.feature.spectral_flatness(y=y).mean())\n",
    "    tonnetz = float(librosa.feature.tonnetz(y=y, sr=sr).mean())\n",
    "    zcr = float(librosa.feature.zero_crossing_rate(y).mean())\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr).mean(axis=1).tolist()\n",
    "\n",
    "    # Improved loudness calculation\n",
    "    S = np.abs(librosa.stft(y))\n",
    "    freqs = librosa.fft_frequencies(sr=sr)\n",
    "    loudness = float(librosa.perceptual_weighting(S**2, freqs).mean())\n",
    "\n",
    "    # Improved key detection\n",
    "    chroma = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
    "    chroma_mean = chroma.mean(axis=1)\n",
    "    key_index = chroma_mean.argmax()\n",
    "    key = librosa.hz_to_note(librosa.midi_to_hz(key_index + 12))  # Convert to musical note\n",
    "\n",
    "    # Calculate danceability\n",
    "    danceability = calculate_danceability(y, sr)\n",
    "\n",
    "    # Calculate instrumentalness\n",
    "    instrumentalness = calculate_instrumentalness(y, sr)\n",
    "\n",
    "    # Calculate energy\n",
    "    energy = calculate_energy(y)\n",
    "\n",
    "    features = {\n",
    "        'Tempo': tempo,\n",
    "        'Loudness': loudness,\n",
    "        'Key': key,\n",
    "        'Danceability': danceability,\n",
    "        'Instrumentalness': instrumentalness,\n",
    "        'Energy': energy\n",
    "    }\n",
    "\n",
    "    # Convert any remaining NumPy arrays to lists and ensure all values are standard Python types\n",
    "    for key, value in features.items():\n",
    "        if isinstance(value, np.ndarray):\n",
    "            features[key] = value.tolist()\n",
    "        elif isinstance(value, (np.float32, np.float64)):\n",
    "            features[key] = float(value)\n",
    "        elif isinstance(value, (np.int32, np.int64)):\n",
    "            features[key] = int(value)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Example usage\n",
    "file_path = r'C:\\Users\\shoir\\is310-coding-assignments\\inclass-code\\Zane Little - Always and Forever.mp3'  # Using raw string\n",
    "# or\n",
    "# file_path = 'C:\\\\Users\\\\shoir\\\\is310-coding-assignments\\\\inclass-code\\\\Zane Little - Always and Forever.mp3'  # Using escaped backslashes\n",
    "\n",
    "audio_features = extract_audio_features(file_path)\n",
    "print(\"Extracted Audio Features:\")\n",
    "print(json.dumps(audio_features, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac01cb3f",
   "metadata": {},
   "source": [
    "### How did we find the Tempo?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa5bd8c",
   "metadata": {},
   "source": [
    "Used the .beat.beat_track function which is used to estimate the tempo whihc is in beats per minute and it detects the beat positions of an audio signal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36757dc1",
   "metadata": {},
   "source": [
    "### How did we find danceability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0d4c7f",
   "metadata": {},
   "source": [
    "To get the danceability we used different features to determine it such as Tempo, Beat Strength (which is the prominence of beats), Spectral Contrast (which is the difference in amplitude between peaks and valleys and a higher spectral contrast indicates a more dynamic and rhythmic music) and specteral flatness (this measures how noise-like the sound is, a lower specteral flatness is typically more danceable). The common variables used are y and sr which are audio time series (y) and sampling rate (sr). We then put the numbers together to get a metric between 0 and 1 to get how danceable the song is. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6282b6e3",
   "metadata": {},
   "source": [
    "### How did we find Loudness?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5262dfb2",
   "metadata": {},
   "source": [
    "To calculate the loudness we used the function perceptual weighting which reflects the human perception of loudness. We first computed the short-time fourier transform which converts the audio signal from the time domeain to a frewquency which hives a spectrogram which can represent the amplitude of different frequencies. Then we computed the power spectrogram which you get by squaring the stft. Then using the perceptual weighting functution it models human hearing hearing sensitivity to different frequencies. After that we get the mean loudness which is our final number from the .mean() function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c6aca5",
   "metadata": {},
   "source": [
    "### How did we find the key of the song?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af19a3c",
   "metadata": {},
   "source": [
    "To get the key, we used the chroma featur which shows the intensity of each of teh 12 different pitches. By averaging it out we ditermined the pitch with the highest average energy, which is most likely the note of the key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08abc691",
   "metadata": {},
   "source": [
    "### How did we get the Instrumentalness?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ac363",
   "metadata": {},
   "source": [
    "To get the instrumentalness we used three steps. We used the Harmonic Percussive Source Seperatione which seperates the signal into the two components. Then we calculated the energy of each component by summing the squared amplitudes. Then after that we calculated the ration of harmonic energy to total energy to give us the instrumentalness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3668e3d3",
   "metadata": {},
   "source": [
    "### How did we get Energy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725ade91",
   "metadata": {},
   "source": [
    "We used the .feature.rms function which calculates the Root Mean Squared energy of an Audo signal. It is typically used to present the perceived energy of the signal. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "is310-new-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
